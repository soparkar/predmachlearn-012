pollutantmean <- function(directory, pollutant, id=1:332) {
#        first <- read.csv("/Users/asoparkar/Documents/coursera/r_programming/specdata/001.csv")
dat <- data.frame()
files_list <- list.files(directory, full.names=TRUE)
for (i in id) {
dat <- rbind(dat, read.csv(files_list[i]))
}
if("nitrate"==pollutant) {
mean(dat$nitrate, na.rm=TRUE)
} else {
mean(dat$sulfate, na.rm=TRUE)
}
}
pollutantmean("/Users/asoparkar/Documents/coursera/r_programming/specdata", "nitrate", 23)
source("/Users/asoparkar/Documents/R_working_directory/pollutantmean.R")
pollutantmean("/Users/asoparkar/Documents/coursera/r_programming/specdata", "nitrate", 23)
corr <- function(directory, threshold = 0) {
dat <- data.frame()
files_list <- list.files(directory, full.names=TRUE)
for (i in 10) {         // CHANGE THIS 10!
dat <- rbind(dat, cor(dat$sulfate, dat$nitrate))
}
dat
}
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/specdata")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/specdata")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/specdata")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/specdata")
source("/Users/asoparkar/Documents/R_working_directory/complete.R")
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 2)
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
corr("/Users/asoparkar/Documents/coursera/r_programming/specdata")
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr2.R")
source("/Users/asoparkar/Documents/R_working_directory/corr2.R")
corr2("/Users/asoparkar/Documents/coursera/r_programming/temp")
corr("/Users/asoparkar/Documents/coursera/r_programming/temp")
source("/Users/asoparkar/Documents/R_working_directory/corr2.R")
corr2("/Users/asoparkar/Documents/coursera/r_programming/specdata")
corr2("/Users/asoparkar/Documents/coursera/r_programming/specdata", 150)
corr2("/Users/asoparkar/Documents/coursera/r_programming/specdata", 400)
source("/Users/asoparkar/Documents/R_working_directory/complete.R")
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 2)
source("/Users/asoparkar/Documents/R_working_directory/complete.R")
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 1)
source("/Users/asoparkar/Documents/R_working_directory/complete.R")
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 1)
source("/Users/asoparkar/Documents/R_working_directory/complete.R")
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 1)
complete <- function(directory, ids=1:332) {
dat <- data.frame()
eachrow <- data.frame()
files_list <- list.files(directory, full.names=TRUE)
for (id in ids) {
nobs <- sum(complete.cases(read.csv(files_list[i])))
eachrow <- cbind(id, nobs)
dat <- rbind(dat, eachrow)
}
dat
}
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 1)
source("/Users/asoparkar/Documents/R_working_directory/complete.R")
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 1)
complete("/Users/asoparkar/Documents/coursera/r_programming/specdata", 30:25)
corr2("/Users/asoparkar/Documents/coursera/r_programming/specdata", 400)
source("/Users/asoparkar/Documents/R_working_directory/corr.R")
set.seed(1)
r.pois(5, 2)
rpois(5, 2)
rpois(5, 4)
library(xlsx)
install.packages("swirl")
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
install.packages("RMySql")
install.packages("RMySQL")
install.packages('RMySQL', type='source')
getwd()
pbinom?
;
?pbinom
pbinom(1, 100, 0.001)
pbinom(2, 100, 0.001)
pbinom(1, 100, 0.001, lower.tail = true)
pbinom(1, 100, 0.001, lower.tail = TRUE)
pbinom(1, 100, 0.001, lower.tail = FALSE)
data(TothGrowth)
library(datasets)
data(TothGrowth)
load(TothGrowth)
load(ToothGrowth)
data(ToothGrowth)
---
title: "ToothGrowth"
author: "Soparkar A"
date: "January 19, 2015"
output: pdf_document
---
1. Load the ToothGrowth data and perform some basic exploratory data analyses
Loading the data
```{r}
data(TothGrowth)
```
Basic exploratory analyses
```{r}
str(ToothGrowth)
```
This synopsis tells us that there are 3 columns and 60 rows in the dataset. It also shows us some sample values.
2. Provide a basic summary of the data.
```{r}
summary(ToothGrowth)
```
3. Use confidence intervals and/or hypothesis tests to compare tooth growth by supp and dose. (Only use the techniques from class, even if there's other approaches worth considering)
and
4. State your conclusions and the assumptions needed for your conclusions.
The exploraion so far tells us that the dataset contains 60 rows, and 3 columns.
Now let us analyze how the length of teeth compares for the two supplement-sources, at various dosage-levels.
```{r}
oj <- subset(ToothGrowth, supp=='OJ')
vc <- subset(ToothGrowth, supp=='VC')
```
Step 1: For doage of 0.5 units
```{r}
oj5 <- subset(oj, dose=='0.5')
vc5 <- subset(vc, dose=='0.5')
as.vector(t.test(oj5$len, vc5$len)$conf.int)
```
We see that both the lower and upper bound of the confidence-interval (obtained usin the t-test here) are positive. This means that for this dosage level, the supplemet of OJ definitely results in longer teeth than its counterpart (VC).
Step 2: For doage of 1.0 units
```{r}
oj10 <- subset(oj, dose=='1')
vc10 <- subset(vc, dose=='1')
as.vector(t.test(oj10$len, vc10$len)$conf.int)
```
We see that both the lower and upper bound of the confidence-interval (obtained usin the t-test here) are positive. This means that for this dosage level, the supplemet of OJ definitely results in longer teeth than its counterpart (VC).
Step 3: For doage of 2.0 units
```{r}
oj20 <- subset(oj, dose=='2')
vc20 <- subset(vc, dose=='2')
as.vector(t.test(oj20$len, vc20$len)$conf.int)
```
We see that for thsi dosage level, the lower bound of confidence-interval (obtained usin the t-test here) is negative while the upper bound is positive. This means that for this dosage-level, none of the supplement methods (OJ or VC) are conclusively better.
Finally, when we compare the two supplement methods overall (ignoring the dosage levels)
```{r}
as.vector(t.test(oj$len, vc$len)$conf.int)
```
We see the same inconclusiveness.
To summarize,
here is the plot from https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/ToothGrowth.html
```{r}
require(graphics)
coplot(len ~ dose | supp, data = ToothGrowth, panel = panel.smooth,
xlab = "ToothGrowth data: length vs dose, given type of supplement")
```
as.vector(t.test(oj5$len, vc5$len)
)
setwd("/Users/asoparkar/Documents/coursera/predmachlearn_012/project")
ls
training = read.csv("/Users/asoparkar/Documents/coursera/predmachlearn_012/project/training.csv")
testing = read.csv("/Users/asoparkar/Documents/coursera/predmachlearn_012/project/testing.csv")
inTrain = createDataPartition(y=training$classe, p=0.7, list=FALSE)
learning = training[inTrain, ]
validating = training[-inTrain, ]
nrow(learning)
nrow(training)
nrow(validating)
inTrain = createDataPartition(y=training$classe, p=0.75, list=FALSE)
library(caret);
library(kernlab);
inTrain = createDataPartition(y=training$classe, p=0.75, list=FALSE)
learning = training[inTrain, ];
validating = training[-inTrain, ];
nrow(learning)
nrow(validating)
modelFit <- train(learning$classe ~ .,method="rf",preProcess="pca",data=learning)
modelFit
modelFit$finalModel
print(modelFit$finalModel)
plot(modelFit$finalModel)
plot(modelFit$finalModel, uniform=TRUE)
library(rattle)
fancyRpartPlot(modelFit$finalModel)
predict(modelFit, validating)
nrow(validating)
ncol(validating)
ncol(learning)
predict(modelFit, validating)
modelFit
randomModelFit <- randomForest(learning$classe ~ ., ntree=3000, preProcess="pca",data=learning)
rfModelFit = modelFit
rpartFit <- train(classe ~ ., preProcess = "pca", data = learning, method = "rpart", tuneLength = 9)
rpartFit
rfModelFit <- train(learning$classe ~ .,method="rf",preProcess="pca",data=learning)
rfModelFit
predTest<-predict(modelFit,validating)
predTest<-predict(modelFit,testing)
training$make
if("make" in colnames(training))
colnames(training)
if("make" %in% colnames(training))
;
"make" %in% colnames(training)
length(which(!is.na(learning$amplitude_pitch_belt)))
length(which(is.na(learning$amplitude_pitch_belt)))
cleaned = learning
ncol(cleaned)
colnames(cleaned)
drops = c("raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "kurtosis_yaw_belt", "skewness_yaw_belt",
"max_roll_belt", "max_picth_belt", "min_pitch_belt", "amplitude_roll_belt", "amplitude_pitch_belt",
"amplitude_yaw_belt", "var_total_accel_belt", "avg_roll_belt", "stddev_roll_belt", "var_roll_belt",
"avg_pitch_belt", "stddev_pitch_belt", "var_pitch_belt", "avg_yaw_belt", "stddev_yaw_belt", "var_yaw_belt",
"var_accel_arm", "avg_roll_arm", "stddev_roll_arm", "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm",
"avg_yaw_arm", "stddev_yaw_arm", "var_yaw_arm", "kurtosis_pitch_arm", "max_roll_arm", "max_pitch_arm",
"max_yaw_arm", "min_roll_arm", "min_pitch_arm", "min_yaw_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"amplitude_yaw_arm", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "max_roll_dumbbell", "max_picth_dumbbell",
"min_roll_dumbbell", "min_pitch_dumbbell", "amplitude_roll_dumbbell", "amplitude_pitch_dumbbell",
"amplitude_yaw_dumbbell", "var_accel_dumbbell", "avg_roll_dumbbell", "stddev_roll_dumbbell", "var_roll_dumbbell",
"avg_pitch_dumbbell", "stddev_pitch_dumbbell", "var_pitch_dumbbell", "avg_yaw_dumbbell", "stddev_yaw_dumbbell",
"var_yaw_dumbbell", "kurtosis_yaw_forearm", "skewness_yaw_forearm", "max_roll_forearm", "max_picth_forearm",
"min_roll_forearm", "min_pitch_forearm", "amplitude_roll_forearm", "amplitude_pitch_forearm", "amplitude_yaw_forearm",
"var_accel_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm", "avg_pitch_forearm",
"stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm", "stddev_yaw_forearm", "var_yaw_forearm")
length(drops)
qq <= learning[, -"magnet_forearm_z"]
qq <= learning[,!(colnames(learning) %in% drops)]
nrow(learning)
nrow(qq)
drops
qq <= learning[,!(names(learning) %in% drops)]
qq = learning[,drops=FALSE]
nrow(qq)
qq = <- subset(learning, select = -drops )
qq <- subset(learning, select = -drops )
qq = subset(learning, select = -drops )
class(learning)
qq <= learning[,!(colnames(learning) %in% drops)]
qq = learning
qq[, drops] <- list(NULL)
ncol(qq)
cleaned = learning;
cleaned[, drops] <- list(NULL)
ncol(cleaned)
modelFit <- train(cleaned$classe ~ ., method="rf")
modelFit <- train(classe ~ ., method="rf", data=cleaned)
library(parallel); library(doParallel)
library(parallel);
library(doParallel);
registerDoParallel(clust <- makeForkCluster(detectCores()));
library(parallel);
library(doParallel);
registerDoParallel(clust <- makeForkCluster(detectCores()));
modelFit <- train(classe ~ ., method="rf", data=cleaned);
stopCluster(clust)
modelFit
predict(cleaned, validating)
predict(modelFit, validating)
prediction <- predict(modelFit, validating)
prediction
validating$classe
modelFit$err.rate
modelFit
modelFit$confusion
prediction <- predict(modelFit, validating)
prediction
ncol(prediction)
length(prediction)
ncol(validating)
class(modelFit)
getTree(modelFit$finalModel, k = 4)
varImp(modelFit)
length(drops)
ncol(cleaned)
table(prediction, validating$classe)
prediction <- predict(modelFit, validating)
prediction
cleaned_validation <- validating;
cleaned_validation[, drops] <- list(NULL);
prediction <- predict(modelFit, cleaned_validation)
prediction
tables(prediction, cleaned_validation$classe)
table(prediction, cleaned_validation$classe)
length(prediction)
length(cleaned_validation)
prediction(1:83)
prediction[1:83]
cleaned_validation$classe
length(cleaned_validation$classe)
length(cleaned_validation)
ncol(cleaned_validation)
nrow(cleaned_validation)
nrow(cleaned)
nrow(validating)
prediction <- predict(modelFit, validating)
length(prediction)
length(validating)
length(validating$classe)
nrow(prediction)
ncol(prediction)
table(prediction, validating$classe))
table(prediction, validating$classe)
predictCrossVal <- predict(model, validating)
predictCrossVal <- predict(modelFit, validating)
confusionMatrix(validating$classe, predictCrossVal)
length(validating$classe)
length(predictCrossVal)
library(caret)
library(kernlab)
library(randomForest)
library(corrplot)
# check if a data folder exists; if not then create one
if (!file.exists("data")) {dir.create("data")}
# file URL and destination file
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
destfile1 <- "./data/pml-training.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
destfile2 <- "./data/pml-testing.csv"
# download the file and note the time
download.file(fileUrl1, destfile = destfile1)
download.file(fileUrl2, destfile = destfile2)
dateDownloaded <- date()
# read the csv file for training
data_training <- read.csv("./data/pml-training.csv", na.strings= c("NA",""," "))
# clean the data by removing columns with NAs etc
data_training_NAs <- apply(data_training, 2, function(x) {sum(is.na(x))})
data_training_clean <- data_training[,which(data_training_NAs == 0)]
# remove identifier columns such as name, timestamps etc
data_training_clean <- data_training_clean[8:length(data_training_clean)]
# split the cleaned testing data into training and cross validation
inTrain <- createDataPartition(y = data_training_clean$classe, p = 0.7, list = FALSE)
training <- data_training_clean[inTrain, ]
crossval <- data_training_clean[-inTrain, ]
# plot a correlation matrix
correlMatrix <- cor(training[, -length(training)])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
# fit a model to predict the classe using everything else as a predictor
model <- randomForest(classe ~ ., data = training)
# crossvalidate the model using the remaining 30% of data
predictCrossVal <- predict(model, crossval)
confusionMatrix(crossval$classe, predictCrossVal)
# apply the same treatment to the final testing data
data_test <- read.csv("./data/pml-testing.csv", na.strings= c("NA",""," "))
data_test_NAs <- apply(data_test, 2, function(x) {sum(is.na(x))})
data_test_clean <- data_test[,which(data_test_NAs == 0)]
data_test_clean <- data_test_clean[8:length(data_test_clean)]
# predict the classes of the test set
predictTest <- predict(model, data_test_clean)
library(caret)
library(kernlab)
library(randomForest)
library(corrplot)
# check if a data folder exists; if not then create one
if (!file.exists("data")) {dir.create("data")}
# file URL and destination file
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
destfile1 <- "./data/pml-training.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
destfile2 <- "./data/pml-testing.csv"
# download the file and note the time
download.file(fileUrl1, destfile = destfile1)
download.file(fileUrl2, destfile = destfile2)
dateDownloaded <- date()
getwd()
destfile1 <- "pml-training.csv"
download.file(fileUrl1, destfile = destfile1)
fileUrl1
training_data = read.csv(file=fileUrl1);
download.file(fileUrl1, destfile = destfile1)
destfile1 <- 'pml-training.csv'
download.file(fileUrl1, destfile = destfile1)
library(RCurl)
setInternet2(use = TRUE)
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
data <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", ssl.verifypeer=0L, followlocation=1L)
writeLines(data,'training.csv')
nrow(training_data)
training_data <- read.csv(text=data)
nrow(training_data)
trdata <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", ssl.verifypeer=0L, followlocation=1L)
training_data <- read.csv(text=trdata)
tsdata <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", ssl.verifypeer=0L, followlocation=1L)
testing_data <- read.csv(text=tsdata)
training_data <- read.csv(text=trdata, na.strings= c("NA",""," "))
# clean the data by removing columns with NAs etc
training_data_NAs <- apply(training_data, 2, function(x) {sum(is.na(x))})
training_data_clean <- training_data[,which(training_data_NAs == 0)]
ncol(training_data)
ncol(training_data_NAs)
ncol(training_data_clean)
training_data_clean <- training_data_clean[8:length(training_data_clean)]
ncol(training_data_clean)
training_data_clean <- training_data[,which(training_data_NAs == 0)]
colnames(training_data_clean)
head(training_data_clean$new_window)
head(training_data_clean$num_window)
training_data_clean <- training_data_clean[8:length(training_data_clean)]
inTrain <- createDataPartition(y = training_data_clean$classe, p = 0.7, list = FALSE)
training <- training_data_clean[inTrain, ]
crossval <- training_data_clean[-inTrain, ]
correlMatrix <- cor(training[, -length(training)])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 200, 0))
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 1, 0))
ncol(training_data_clean)
model <- randomForest(classe ~ ., data = training)
library(parallel);
library(doParallel);
registerDoParallel(clust <- makeForkCluster(detectCores()));
model <- randomForest(classe ~ ., data = training)
stopCluster(clust);
model
modelFit = model
predictCrossVal <- predict(modelFit, crossval)
confusionMatrix(crossval$classe, predictCrossVal)
testing_data_NAs <- apply(testing_data, 2, function(x) {sum(is.na(x))})
testing_data_clean <- testing_data[,which(testing_data_NAs == 0)]
testing_data_clean <- testing_data_clean[8:length(testing_data_clean)]
ncol(testing_data_clean)
ncol(modelFit)
length(modelFit)
predictTest <- predict(modelFit, testing_data_clean)
predictTest
confusionMatrix(testing_data_clean$classe, predictTest)
library(caret)
library(kernlab)
library(randomForest)
library(corrplot)
library(RCurl)
# read the data
trdata <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", ssl.verifypeer=0L, followlocation=1L)
training_data <- read.csv(text=trdata, na.strings= c("NA",""," "))
tsdata <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", ssl.verifypeer=0L, followlocation=1L)
testing_data <- read.csv(text=tsdata)
# remove columns with NAs
training_data_NAs <- apply(training_data, 2, function(x) {sum(is.na(x))})
training_data_clean <- training_data[,which(training_data_NAs == 0)]
# remove identifier columns such as names and timestamps
training_data_clean <- training_data_clean[8:length(training_data_clean)]
# split the cleaned testing data into training and cross validation
inTrain <- createDataPartition(y = training_data_clean$classe, p = 0.7, list = FALSE)
training <- training_data_clean[inTrain, ]
cv <- training_data_clean[-inTrain, ]
# plot a correlation matrix
correlMatrix <- cor(training[, -length(training)])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 1, 0))
# fit a model to predict the classe using everything else as a predictor
library(parallel);
library(doParallel);
registerDoParallel(clust <- makeForkCluster(detectCores()));
modelFit <- randomForest(classe ~ ., data = training)
stopCluster(clust);
# cross-validate the model using the remaining 30% of data
predictCrossVal <- predict(modelFit, cv)
confusionMatrix(cv$classe, predictCrossVal)
# clean the final testing data in the same way
testing_data_NAs <- apply(testing_data, 2, function(x) {sum(is.na(x))})
testing_data_clean <- testing_data[,which(testing_data_NAs == 0)]
testing_data_clean <- testing_data_clean[8:length(testing_data_clean)]
# predict the classes of the test set
predictTest <- predict(modelFit, testing_data_clean)
predictTest
confusionMatrix
confusionMatrix(cv$classe, predictCrossVal)
modelFit
modelFit$err
modelFit$err.rate
modelFit
summary(modelFit)
summary(modelFit$err.rate)
mean(modelFit$err.rate)
```{r, warning=FALSE}
ncol(training_data_clean)
ncol(training_data)
modelFit
predictTest
modelFit
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
predictTest
length(predictTest)
nrow(predictTest)
ncol(predictTest)
temp = rep("A", 20)
temp
getwd()
pml_write_files(predictTest)
